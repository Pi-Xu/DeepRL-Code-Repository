{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QizpiHDh9Fwk"
      },
      "source": [
        "## Editing Code\n",
        "\n",
        "To edit code, click the folder icon on the left menu. Navigate to the corresponding file (`cs285_f2021/...`). Double click a file to open an editor. There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window). We sync your edits to Google Drive so that you won't lose your work in the event of an instance timeout, but you will need to re-mount your Google Drive and re-install packages with every new instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_OxQ1AZSyXC"
      },
      "source": [
        "## Run Actor Critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "both",
        "id": "IzuN647wT9iJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\SF\\Anaconda\\envs\\rlenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "#@title imports\n",
        "import os\n",
        "import time\n",
        "\n",
        "from cs285.agents.ac_agent import ACAgent\n",
        "from cs285.infrastructure.rl_trainer import RL_Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "both",
        "id": "PQ9qWQu7TNb9"
      },
      "outputs": [],
      "source": [
        "#@title runtime arguments\n",
        "\n",
        "class ACArgs:\n",
        "\n",
        "  def __getitem__(self, key):\n",
        "    return getattr(self, key)\n",
        "\n",
        "  def __setitem__(self, key, val):\n",
        "    setattr(self, key, val)\n",
        "\n",
        "  def __contains__(self, key):\n",
        "    return hasattr(self, key)\n",
        "\n",
        "  env_name = 'CartPole-v0' #@param ['CartPole-v0', 'InvertedPendulum-v2', 'HalfCheetah-v2']\n",
        "  exp_name = 'q4_ac_1_1' #@param\n",
        "\n",
        "  ## PDF will tell you how to set ep_len\n",
        "  ## and discount for each environment\n",
        "  ep_len = 200 #@param {type: \"integer\"}\n",
        "\n",
        "  #@markdown batches and steps\n",
        "  batch_size = 1000 #@param {type: \"integer\"}\n",
        "  eval_batch_size =  400#@param {type: \"integer\"}\n",
        "\n",
        "  n_iter = 100 #@param {type: \"integer\"}\n",
        "  num_agent_train_steps_per_iter = 1 #@param {type: \"integer\"}\n",
        "  num_actor_updates_per_agent_update = 1 #@param {type: \"integer\"}\n",
        "  num_critic_updates_per_agent_update = 1 #@param {type: \"integer\"}\n",
        "  \n",
        "  #@markdown Actor-Critic parameters\n",
        "  discount =  0.9#@param {type: \"number\"}\n",
        "  learning_rate = 5e-3 #@param {type: \"number\"}\n",
        "  dont_standardize_advantages = False #@param {type: \"boolean\"}\n",
        "  num_target_updates = 10 #@param {type: \"integer\"}\n",
        "  num_grad_steps_per_target_update = 10 #@param {type: \"integer\"}\n",
        "  n_layers = 2 #@param {type: \"integer\"}\n",
        "  size = 64 #@param {type: \"integer\"}\n",
        "\n",
        "  #@markdown system\n",
        "  save_params = False #@param {type: \"boolean\"}\n",
        "  no_gpu = False #@param {type: \"boolean\"}\n",
        "  which_gpu = 0 #@param {type: \"integer\"}\n",
        "  seed = 1 #@param {type: \"integer\"}\n",
        "\n",
        "  #@markdown logging\n",
        "  ## default is to not log video so\n",
        "  ## that logs are small enough to be\n",
        "  ## uploaded to gradscope\n",
        "  video_log_freq =  -1#@param {type: \"integer\"}\n",
        "  scalar_log_freq = 10 #@param {type: \"integer\"}\n",
        "\n",
        "\n",
        "args = ACArgs()\n",
        "\n",
        "\n",
        "if args['video_log_freq'] > 0:\n",
        "  import warnings\n",
        "  warnings.warn(\n",
        "      '''\\nLogging videos will make eventfiles too'''\n",
        "      '''\\nlarge for the autograder. Set video_log_freq = -1'''\n",
        "      '''\\nfor the runs you intend to submit.''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "both",
        "id": "wqUVP5E5S1z8"
      },
      "outputs": [],
      "source": [
        "#@title Define AC trainer\n",
        "\n",
        "class AC_Trainer(object):\n",
        "\n",
        "    def __init__(self, params):\n",
        "\n",
        "        #####################\n",
        "        ## SET AGENT PARAMS\n",
        "        #####################\n",
        "\n",
        "        computation_graph_args = {\n",
        "            'n_layers': params['n_layers'],\n",
        "            'size': params['size'],\n",
        "            'learning_rate': params['learning_rate'],\n",
        "            'num_target_updates': params['num_target_updates'],\n",
        "            'num_grad_steps_per_target_update': params['num_grad_steps_per_target_update'],\n",
        "            }\n",
        "\n",
        "        estimate_advantage_args = {\n",
        "            'gamma': params['discount'],\n",
        "            'standardize_advantages': not(params['dont_standardize_advantages']),\n",
        "        }\n",
        "\n",
        "        train_args = {\n",
        "            'num_agent_train_steps_per_iter': params['num_agent_train_steps_per_iter'],\n",
        "            'num_critic_updates_per_agent_update': params['num_critic_updates_per_agent_update'],\n",
        "            'num_actor_updates_per_agent_update': params['num_actor_updates_per_agent_update'],\n",
        "        }\n",
        "\n",
        "        agent_params = {**computation_graph_args, **estimate_advantage_args, **train_args}\n",
        "\n",
        "        self.params = params\n",
        "        self.params['agent_class'] = ACAgent\n",
        "        self.params['agent_params'] = agent_params\n",
        "        self.params['train_batch_size'] = params['batch_size']\n",
        "        self.params['batch_size_initial'] = self.params['batch_size']\n",
        "        self.params['non_atari_colab_env'] = True\n",
        "\n",
        "        ################\n",
        "        ## RL TRAINER\n",
        "        ################\n",
        "\n",
        "        self.rl_trainer = RL_Trainer(self.params)\n",
        "\n",
        "    def run_training_loop(self):\n",
        "\n",
        "        self.rl_trainer.run_training_loop(\n",
        "            self.params['n_iter'],\n",
        "            collect_policy = self.rl_trainer.agent.actor,\n",
        "            eval_policy = self.rl_trainer.agent.actor,\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xuNw8N1jTg1p"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOGGING TO:  D:\\Code\\RL-homework\\hw3\\data\\q4_ac_1_1_CartPole-v0_06-10-2022_18-42-58\n"
          ]
        }
      ],
      "source": [
        "#@title create directories for logging\n",
        "\n",
        "\n",
        "data_path =r'D:\\Code\\RL-homework\\hw3\\data'\n",
        "\n",
        "if not (os.path.exists(data_path)):\n",
        "    os.makedirs(data_path)\n",
        "\n",
        "logdir = args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join(data_path, logdir)\n",
        "args['logdir'] = logdir\n",
        "if not(os.path.exists(logdir)):\n",
        "    os.makedirs(logdir)\n",
        "\n",
        "print(\"LOGGING TO: \", logdir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_IGogH9YTt1y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "########################\n",
            "logging outputs to  D:\\Code\\RL-homework\\hw3\\data\\q4_ac_1_1_CartPole-v0_06-10-2022_18-42-58\n",
            "########################\n",
            "Using GPU id 0\n",
            "Sequential(\n",
            "  (0): Linear(in_features=4, out_features=64, bias=True)\n",
            "  (1): Tanh()\n",
            "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (3): Tanh()\n",
            "  (4): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (5): Identity()\n",
            ")\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\SF\\Anaconda\\envs\\rlenv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([1, 1000])) that is different to the input size (torch.Size([1000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 28.928571701049805\n",
            "Eval_StdReturn : 13.935894012451172\n",
            "Eval_MaxReturn : 71.0\n",
            "Eval_MinReturn : 13.0\n",
            "Eval_AverageEpLen : 28.928571428571427\n",
            "Train_AverageReturn : 27.648649215698242\n",
            "Train_StdReturn : 12.643160820007324\n",
            "Train_MaxReturn : 65.0\n",
            "Train_MinReturn : 11.0\n",
            "Train_AverageEpLen : 27.64864864864865\n",
            "Train_EnvstepsSoFar : 1023\n",
            "TimeSinceStart : 3.2937779426574707\n",
            "Critic_Loss : 1.2004905939102173\n",
            "Actor_Loss : -0.0007392740808427334\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "\n",
            "********** Iteration 5 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "\n",
            "********** Iteration 6 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "\n",
            "********** Iteration 7 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "\n",
            "********** Iteration 8 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "\n",
            "********** Iteration 9 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "\n",
            "********** Iteration 10 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 26.6875\n",
            "Eval_StdReturn : 7.759822368621826\n",
            "Eval_MaxReturn : 47.0\n",
            "Eval_MinReturn : 16.0\n",
            "Eval_AverageEpLen : 26.6875\n",
            "Train_AverageReturn : 36.0\n",
            "Train_StdReturn : 23.31768226623535\n",
            "Train_MaxReturn : 109.0\n",
            "Train_MinReturn : 14.0\n",
            "Train_AverageEpLen : 36.0\n",
            "Train_EnvstepsSoFar : 11241\n",
            "TimeSinceStart : 14.408835411071777\n",
            "Critic_Loss : 1.4096051454544067\n",
            "Actor_Loss : -0.00011995220847893506\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 11 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "\n",
            "********** Iteration 12 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32md:\\Code\\RL-homework\\hw3\\run_hw3_actor_critic.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/RL-homework/hw3/run_hw3_actor_critic.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#@title run training\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/RL-homework/hw3/run_hw3_actor_critic.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer \u001b[39m=\u001b[39m AC_Trainer(args)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Code/RL-homework/hw3/run_hw3_actor_critic.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mrun_training_loop()\n",
            "\u001b[1;32md:\\Code\\RL-homework\\hw3\\run_hw3_actor_critic.ipynb Cell 7\u001b[0m in \u001b[0;36mAC_Trainer.run_training_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/RL-homework/hw3/run_hw3_actor_critic.ipynb#W6sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_training_loop\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Code/RL-homework/hw3/run_hw3_actor_critic.ipynb#W6sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrl_trainer\u001b[39m.\u001b[39;49mrun_training_loop(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/RL-homework/hw3/run_hw3_actor_critic.ipynb#W6sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mn_iter\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/RL-homework/hw3/run_hw3_actor_critic.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m         collect_policy \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrl_trainer\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mactor,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/RL-homework/hw3/run_hw3_actor_critic.ipynb#W6sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m         eval_policy \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrl_trainer\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mactor,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/RL-homework/hw3/run_hw3_actor_critic.ipynb#W6sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m         )\n",
            "File \u001b[1;32md:\\Code\\RL-homework\\hw3\\cs285\\infrastructure\\rl_trainer.py:185\u001b[0m, in \u001b[0;36mRL_Trainer.run_training_loop\u001b[1;34m(self, n_iter, collect_policy, eval_policy, initial_expertdata, relabel_with_expert, start_relabel_with_expert, expert_policy)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m itr \u001b[39m%\u001b[39m print_period \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    184\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mTraining agent...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 185\u001b[0m all_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_agent()\n\u001b[0;32m    187\u001b[0m \u001b[39m# log/save\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_video \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogmetrics:\n\u001b[0;32m    189\u001b[0m     \u001b[39m# perform logging\u001b[39;00m\n",
            "File \u001b[1;32md:\\Code\\RL-homework\\hw3\\cs285\\infrastructure\\rl_trainer.py:259\u001b[0m, in \u001b[0;36mRL_Trainer.train_agent\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m     ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39msample(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mtrain_batch_size\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    256\u001b[0m     \u001b[39m# TODO use the sampled data to train an agent\u001b[39;00m\n\u001b[0;32m    257\u001b[0m     \u001b[39m# HINT: use the agent's train function\u001b[39;00m\n\u001b[0;32m    258\u001b[0m     \u001b[39m# HINT: keep the agent's training log for debugging\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m     train_log \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mtrain(ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch)\n\u001b[0;32m    260\u001b[0m     all_logs\u001b[39m.\u001b[39mappend(train_log)\n\u001b[0;32m    261\u001b[0m \u001b[39mreturn\u001b[39;00m all_logs\n",
            "File \u001b[1;32md:\\Code\\RL-homework\\hw3\\cs285\\agents\\ac_agent.py:44\u001b[0m, in \u001b[0;36mACAgent.train\u001b[1;34m(self, ob_no, ac_na, re_n, next_ob_no, terminal_n)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, ob_no, ac_na, re_n, next_ob_no, terminal_n):\n\u001b[0;32m     34\u001b[0m     \u001b[39m# TODO Implement the following pseudocode:\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[39m# for agent_params['num_critic_updates_per_agent_update'] steps,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[39m# for agent_params['num_actor_updates_per_agent_update'] steps,\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[39m#     update the actor\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     loss \u001b[39m=\u001b[39m OrderedDict()\n\u001b[1;32m---> 44\u001b[0m     loss[\u001b[39m'\u001b[39m\u001b[39mCritic_Loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic\u001b[39m.\u001b[39;49mupdate(ob_no, ac_na, next_ob_no, re_n,  terminal_n)\n\u001b[0;32m     46\u001b[0m     adv_n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimate_advantage(ob_no, next_ob_no, re_n, terminal_n)\n\u001b[0;32m     48\u001b[0m     loss[\u001b[39m'\u001b[39m\u001b[39mActor_Loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39mupdate(ob_no, ac_na, adv_n)\n",
            "File \u001b[1;32md:\\Code\\RL-homework\\hw3\\cs285\\critics\\bootstrapped_continuous_critic.py:111\u001b[0m, in \u001b[0;36mBootstrappedContinuousCritic.update\u001b[1;34m(self, ob_no, ac_na, next_ob_no, reward_n, terminal_n)\u001b[0m\n\u001b[0;32m    108\u001b[0m v_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic_network(ob_no)\n\u001b[0;32m    109\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(v_t, targets)\n\u001b[1;32m--> 111\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[0;32m    112\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m    113\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[1;32md:\\SF\\Anaconda\\envs\\rlenv\\lib\\site-packages\\torch\\optim\\optimizer.py:206\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_zero_grad_profile_name\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_for_profile()\n\u001b[1;32m--> 206\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49mrecord_function(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_zero_grad_profile_name):\n\u001b[0;32m    207\u001b[0m     \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[0;32m    208\u001b[0m         \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m group[\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m]:\n",
            "File \u001b[1;32md:\\SF\\Anaconda\\envs\\rlenv\\lib\\site-packages\\torch\\autograd\\profiler.py:432\u001b[0m, in \u001b[0;36mrecord_function.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_callbacks_on_exit: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[39m# Stores underlying RecordFunction as a tensor. TODO: move to custom\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[39m# class (https://github.com/pytorch/pytorch/issues/35026).\u001b[39;00m\n\u001b[1;32m--> 432\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(\u001b[39m1\u001b[39;49m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title run training\n",
        "trainer = AC_Trainer(args)\n",
        "trainer.run_training_loop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjhrgXnUTzyi"
      },
      "outputs": [],
      "source": [
        "#@markdown You can visualize your runs with tensorboard from within the notebook\n",
        "\n",
        "## requires tensorflow==2.3.0\n",
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir /content/cs285_f2021/homework_fall2021/hw3/data/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "run_hw3_actor_critic.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 ('rlenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "e99c6a051cabfccea28eda58226a3deb0006c283efe5463cd051c492fa4b03c9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
